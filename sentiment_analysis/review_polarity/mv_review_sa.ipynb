{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "data_path = 'txt_sentoken/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "def load_doc(fpath):\n",
    "    text = ''\n",
    "    with open(fpath, 'r') as infile:\n",
    "        text = infile.read()\n",
    "    return text\n",
    "def clean_text(text):\n",
    "    tokens = text.split()\n",
    "    table = str.maketrans('', '', punctuation) # remove punctuation\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [w for w in tokens if w.isalpha() and len(w) > 1] #remove numbers, 1 letter words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if w not in stop_words] \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_vocab(fpath, vocab):\n",
    "    text = load_doc(fpath)\n",
    "    tokens = clean_text(text)\n",
    "    vocab.update(tokens)\n",
    "\n",
    "def build_vocab(dir_path, vocab, is_train):\n",
    "    for fname in os.listdir(dir_path):\n",
    "        if is_train and fname.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not fname.startswith('cv9'):\n",
    "            continue\n",
    "        fpath = dir_path + '/' + fname\n",
    "        update_vocab(fpath, vocab)\n",
    "\n",
    "def process_doc(dir_path, is_train):\n",
    "    docs = []\n",
    "    for fname in os.listdir(dir_path):\n",
    "        if is_train and fname.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not fname.startswith('cv9'):\n",
    "            continue\n",
    "        fpath = dir_path + '/' + fname\n",
    "        text = load_doc(fpath)\n",
    "        tokens = clean_text(text)\n",
    "        docs.append(' '.join(tokens))\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_file(fpath, outlist):\n",
    "    with open(fpath, 'w') as outfile:\n",
    "        if type(outlist[0]) == str:\n",
    "            data = '\\n'.join(outlist)\n",
    "        else:\n",
    "            data = '\\n'.join(['\\t'.join(item) for item in outlist])\n",
    "        outfile.write(data)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32487\n",
      "44276\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "vocab = Counter()\n",
    "build_vocab(data_path+'pos', vocab, is_train=True)\n",
    "print(len(vocab))\n",
    "build_vocab(data_path+ 'neg', vocab, is_train=True)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844)]\n"
     ]
    }
   ],
   "source": [
    "print(vocab.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25767\n"
     ]
    }
   ],
   "source": [
    "#filter words occurring less than n = 1 times\n",
    "min_count = 2\n",
    "vocab_tokens = [k for k,v in vocab.items() if v >= min_count]\n",
    "print(len(vocab_tokens))\n",
    "#save the vocabulary\n",
    "list_to_file('vocabulary.txt', vocab_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_docs = process_doc(data_path+'pos', is_train=True)\n",
    "print(len(pos_docs))\n",
    "neg_docs = process_doc(data_path+'neg', is_train=True)\n",
    "print(len(neg_docs))\n",
    "train_docs = pos_docs + neg_docs\n",
    "len(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-a293dc8f2b09>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-a293dc8f2b09>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from keras.preprocessing import text.Tokenizer, sequence.pad_sequences\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs_encd = tokenizer.texts_to_sequences(train_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1380\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_len = max([len(d.split()) for d in train_docs])\n",
    "X_train = pad_sequences(train_docs_encd, maxlen=max_len, padding='post')\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array([1 for i in range(900)] + [0 for i in range(900)])\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "200\n",
      "200\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "#process test data\n",
    "pos_docs1 = process_doc(data_path+'pos/', is_train=False)\n",
    "print(len(pos_docs1))\n",
    "neg_docs1 = process_doc(data_path+'neg/', is_train=False)\n",
    "print(len(neg_docs1))\n",
    "test_docs = pos_docs1 + neg_docs1\n",
    "print(len(test_docs))\n",
    "test_docs_encd = tokenizer.texts_to_sequences(test_docs)\n",
    "X_test = pad_sequences(test_docs_encd, maxlen=max_len, padding='post')\n",
    "print(len(X_test))\n",
    "y_test = np.array([1 for i in range(len(pos_docs1))] + [0 for i in range(len(neg_docs1))])\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44277"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1 # additional 1 for unknown words\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Conv1D, MaxPool1D,Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________________________\n",
      "Layer (type)                   Output Shape                Param #    \n",
      "======================================================================\n",
      "embedding_2 (Embedding)        (None, 1380, 100)           4427700    \n",
      "______________________________________________________________________\n",
      "conv1d_2 (Conv1D)              (None, 1373, 32)            25632      \n",
      "______________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D) (None, 686, 32)             0          \n",
      "______________________________________________________________________\n",
      "flatten_2 (Flatten)            (None, 21952)               0          \n",
      "______________________________________________________________________\n",
      "dense_3 (Dense)                (None, 10)                  219530     \n",
      "______________________________________________________________________\n",
      "dense_4 (Dense)                (None, 1)                   11         \n",
      "======================================================================\n",
      "Total params: 4,672,873\n",
      "Trainable params: 4,672,873\n",
      "Non-trainable params: 0\n",
      "______________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_len))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2)) #reduce the size of conv layer by half\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary(70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "36s - loss: 0.6908 - acc: 0.5261\n",
      "Epoch 2/5\n",
      "22s - loss: 0.5670 - acc: 0.7483\n",
      "Epoch 3/5\n",
      "21s - loss: 0.1268 - acc: 0.9939\n",
      "Epoch 4/5\n",
      "21s - loss: 0.0060 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "21s - loss: 0.0023 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f272db69898>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=5, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss =  0.438384304047\n",
      "Test accuracy =  82.0\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss = \", loss)\n",
    "print(\"Test accuracy = \", accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
